<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/9287dd2f6369792d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-35a6f84fb05eb603.js"/><script src="/_next/static/chunks/fd9d1056-182ecd18cdd342a6.js" async=""></script><script src="/_next/static/chunks/117-170334a637c69be7.js" async=""></script><script src="/_next/static/chunks/main-app-ca5f9dd76f7bb0ca.js" async=""></script><title>DocRAG - Instant RAG for Your Docs</title><meta name="description" content="The fastest way to build RAG over any documentation"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_299a82 font-sans bg-background"><script src="/_next/static/chunks/webpack-35a6f84fb05eb603.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/1270b87bc1d0b22c-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/24b41f8b8dc75a69-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/media/2e1ce5028a1ee136-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n4:HL[\"/_next/static/media/58ff31a0bb2b9f19-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n5:HL[\"/_next/static/media/62f0bf4663640416-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n6:HL[\"/_next/static/media/8032c0b6bc1a3d69-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n7:HL[\"/_next/static/media/870d02ea8f2d2cf7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n8:HL[\"/_next/static/media/b5cfbd14ed23535c-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n9:HL[\"/_next/static/media/f1ff65ca9ee570b6-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\na:HL[\"/_next/static/media/fef545d57c98d742-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\nb:HL[\"/_next/static/css/9287dd2f6369792d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"c:I[2846,[],\"\"]\nf:I[4707,[],\"\"]\n11:I[6423,[],\"\"]\n12:I[4698,[\"317\",\"static/chunks/317-e85949e7f5e6373a.js\",\"175\",\"static/chunks/175-398a4546003db254.js\",\"749\",\"static/chunks/749-19b41e0c3154689b.js\",\"185\",\"static/chunks/app/layout-b7f4d8fcecfc638b.js\"],\"Providers\"]\n13:I[9536,[\"317\",\"static/chunks/317-e85949e7f5e6373a.js\",\"175\",\"static/chunks/175-398a4546003db254.js\",\"749\",\"static/chunks/749-19b41e0c3154689b.js\",\"185\",\"static/chunks/app/layout-b7f4d8fcecfc638b.js\"],\"Cursor\"]\n15:I[1060,[],\"\"]\n10:[\"slug\",\"building-rag-with-langchain\",\"d\"]\n16:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$Lc\",null,{\"buildId\":\"5zDkaiKMhkypkouUNiOt7\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"blog\",\"building-rag-with-langchain\"],\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"building-rag-with-langchain\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"building-rag-with-langchain\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"building-rag-with-langchain\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$Ld\",\"$Le\",null],null],null]},[null,[\"$\",\"$Lf\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$10\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L11\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$Lf\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L11\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/9287dd2f6369792d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"className\":\"__variable_299a82 font-sans bg-background\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"$L12\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L11\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}],[\"$\",\"$L13\",null,{}]]}]}]]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$L14\"],\"globalErrorComponent\":\"$15\",\"missingSlots\":\"$W16\"}]\n"])</script><script>self.__next_f.push([1,"17:I[1919,[\"317\",\"static/chunks/317-e85949e7f5e6373a.js\",\"175\",\"static/chunks/175-398a4546003db254.js\",\"739\",\"static/chunks/739-9ed3278642e8fb5f.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-ae8a35813dbe64d4.js\"],\"BlogNavbar\"]\n18:T1008,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eBuilding RAG Systems with LangChain\u003c/h1\u003e\n\u003cp\u003eRetrieval Augmented Generation (RAG) is a powerful technique that enhances Large Language Models (LLMs) by providing them with relevant context from your documentation. In this tutorial, we'll learn how to build a RAG system using LangChain.\u003c/p\u003e\n\u003ch2\u003eWhat is RAG?\u003c/h2\u003e\n\u003cp\u003eRAG combines the power of retrieval systems with generative AI. Instead of relying solely on the LLM's training data, RAG retrieves relevant documents from your knowledge base and uses them as context for generating responses.\u003c/p\u003e\n\u003ch2\u003eSetting Up LangChain\u003c/h2\u003e\n\u003cp\u003eFirst, install the required dependencies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003enpm install langchain @langchain/openai\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eCreating the Document Store\u003c/h2\u003e\n\u003cp\u003eLet's create a simple document store using LangChain's document loaders:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-typescript\"\u003eimport { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nconst loader = new DirectoryLoader(\n  \"./docs\",\n  {\n    \".txt\": (path) =\u003e new TextLoader(path),\n  }\n);\n\nconst docs = await loader.load();\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eSetting Up the Vector Store\u003c/h2\u003e\n\u003cp\u003eNext, we'll create embeddings and store them in a vector database:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-typescript\"\u003eimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await MemoryVectorStore.fromDocuments(\n  docs,\n  embeddings\n);\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eCreating the RAG Chain\u003c/h2\u003e\n\u003cp\u003eNow we can create a RAG chain that combines retrieval with generation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-typescript\"\u003eimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createRetrievalChain } from \"langchain/chains/retrieval\";\nimport { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the following question using only the provided context. If you cannot\nanswer the question with the context, say \"I don't have enough information.\"\n\nContext: {context}\n\nQuestion: {input}\n`);\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-4\",\n  temperature: 0.7,\n});\n\nconst documentChain = await createStuffDocumentsChain({\n  llm: model,\n  prompt,\n});\n\nconst retriever = vectorStore.asRetriever();\nconst retrievalChain = await createRetrievalChain({\n  combineDocsChain: documentChain,\n  retriever,\n});\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eUsing the RAG System\u003c/h2\u003e\n\u003cp\u003eFinally, we can use our RAG system to answer questions:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-typescript\"\u003econst response = await retrievalChain.invoke({\n  input: \"How do I deploy to production?\",\n});\n\nconsole.log(response.answer);\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eBest Practices\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDocument Chunking\u003c/strong\u003e: Choose appropriate chunk sizes for your documents. Too large chunks can dilute relevance, while too small chunks might lose context.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEmbeddings\u003c/strong\u003e: Use embeddings that are appropriate for your domain. OpenAI's embeddings work well for general text, but you might want specialized embeddings for technical documentation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePrompt Engineering\u003c/strong\u003e: Carefully design your prompts to guide the model in using the retrieved context effectively.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: Regularly evaluate your RAG system's performance using metrics like relevance and answer accuracy.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eRAG is a powerful way to enhance LLMs with your own documentation. By following this tutorial, you've learned how to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSet up a document store with LangChain\u003c/li\u003e\n\u003cli\u003eCreate embeddings and store them in a vector database\u003c/li\u003e\n\u003cli\u003eBuild a RAG chain that combines retrieval and generation\u003c/li\u003e\n\u003cli\u003eUse the system to answer questions about your documentation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor more advanced topics, check out LangChain's documentation on streaming responses, custom retrievers, and advanced RAG patterns.\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"e:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background\",\"children\":[[\"$\",\"$L17\",null,{}],[\"$\",\"main\",null,{\"className\":\"container mx-auto px-4 py-24\",\"children\":[\"$\",\"article\",null,{\"className\":\"prose prose-lg dark:prose-invert mx-auto\",\"children\":[[\"$\",\"header\",null,{\"className\":\"text-center mb-16 not-prose\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold tracking-tight gradient-text sm:text-5xl mb-4\",\"children\":\"Building RAG Systems with LangChain\"}],[\"$\",\"time\",null,{\"className\":\"text-muted-foreground\",\"children\":\"2024-01-10\"}]]}],[\"$\",\"div\",null,{\"className\":\"markdown\",\"dangerouslySetInnerHTML\":{\"__html\":\"$18\"}}]]}]}]]}]\n14:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"DocRAG - Instant RAG for Your Docs\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"The fastest way to build RAG over any documentation\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\nd:null\n"])</script></body></html>